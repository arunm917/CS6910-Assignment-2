{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VzAneOTgpS22",
        "plK67r6KS6Cw",
        "9Fpg0RpTd1OD",
        "p9TXE3qzi_52"
      ],
      "mount_file_id": "1WATHOk4JB_iQsrx8mlR2MYjiVzpn_uUj",
      "authorship_tag": "ABX9TyNy/Wlp8sjMsQCEPflzjtWG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunm917/CS6910-Assignment-2/blob/main/CS6910_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing packages and assigning directory"
      ],
      "metadata": {
        "id": "VzAneOTgpS22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq"
      ],
      "metadata": {
        "id": "1ex7xikhoWx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c562df9-1a09-4d90-e166-712cbfdf1d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31JWYXO7MqfJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "from torchvision.datasets import ImageFolder\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import ConcatDataset\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules.pooling import MaxPool2d\n",
        "import torch.optim as optim\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " wandb.login()"
      ],
      "metadata": {
        "id": "lxAWD-4Pp9kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wosKLQXqM3Vd",
        "outputId": "094e222e-98c4-46e7-aea2-d6af8c9b016f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/drive/Othercomputers/My Computer/Courses/CS6910_2023/CS6910-Assignment-2/inaturalist_12K/train'\n",
        "test_path = '/content/drive/Othercomputers/My Computer/Courses/CS6910_2023/CS6910-Assignment-2/inaturalist_12K/val'"
      ],
      "metadata": {
        "id": "YPNTufBrNYfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxfKPrtOvgXs",
        "outputId": "45c15207-c51e-4b7f-b5e0-21b37b77398f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "gNV0TiOcv1m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing dataset"
      ],
      "metadata": {
        "id": "ckceWjI7du2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "V4B0mZlZikgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "ZkkUIPmIini9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = ImageFolder(train_path, transform=train_transforms)\n",
        "train_size = int(0.8*len(train_data))\n",
        "val_size = len(train_data) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_data, [train_size, val_size])\n",
        "test_dataset = ImageFolder(test_path, transform=test_transforms)"
      ],
      "metadata": {
        "id": "HFzJcDg6bhmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DataLoader(train_dataset, shuffle=True, num_workers = 2)\n",
        "dataiter = iter(dataset)\n",
        "images, labels = next(dataiter)"
      ],
      "metadata": {
        "id": "DD3z-c-y1qRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape)\n",
        "image_dim = images.shape[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzGEboz82EaB",
        "outputId": "89f63198-0310-4644-fe9e-3d3cdf6ac099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data visualization"
      ],
      "metadata": {
        "id": "9Fpg0RpTd1OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = 30, shuffle=True, num_workers = 2)\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)"
      ],
      "metadata": {
        "id": "yHd7qiim5GNJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.utils as vutils\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize the image\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Get a batch of images from the train loader\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show a few images from the batch\n",
        "imshow(torchvision.utils.make_grid(images, nrow = 10))\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "zvJn5HG2d4Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRVTl8Qf2pao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Architecture"
      ],
      "metadata": {
        "id": "vzExef8VrThp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_output_dim(num_filters,num_layers, conv_filter_size, maxpool_filter_size):\n",
        "    input_size = image_dim\n",
        "    for i in range (num_layers):\n",
        "      conv_output_dim = (input_size - conv_filter_size) + 1\n",
        "      print('conv_output',conv_output_dim)\n",
        "      maxpool_output_dim = np.floor((conv_output_dim - maxpool_filter_size)/2) + 1\n",
        "      input_size = maxpool_output_dim\n",
        "      print('maxpool_output',maxpool_output_dim)\n",
        "    return int(maxpool_output_dim**2*num_filters)"
      ],
      "metadata": {
        "id": "-_GofBrPN9Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_output_dim(16,5, 4, 2)"
      ],
      "metadata": {
        "id": "c5689OEcN980"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, architecture, batch_norm, num_layers, num_filters, conv_filter_size, dropout, activation, dense_neurons):\n",
        "    super(CNN, self).__init__()\n",
        "    \n",
        "    self.layers = []\n",
        "    self.architecture = architecture\n",
        "    self.batch_norm = batch_norm\n",
        "    self.num_conv_layers = num_layers\n",
        "    self.num_filters = num_filters\n",
        "    self.conv_filter_size = conv_filter_size\n",
        "    self.dropout = dropout\n",
        "    self.dense_neurons = dense_neurons\n",
        "    self.input_filters = 3\n",
        "    self.maxpool_filter_size = 2\n",
        "\n",
        "    if activation == 'ReLU':\n",
        "      self.activation = nn.ReLU\n",
        "    if activation == 'GELU':\n",
        "      self.activation = nn.GELU\n",
        "    if activation == \"SiLU\":\n",
        "      self.activation = nn.SiLU\n",
        "    if activation == \"Mish\":\n",
        "      self.activation = nn.Mish\n",
        "    if activation == 'LeakyReLU':\n",
        "      self.activation = nn.LeakyReLU\n",
        "    if activation == 'ELU':\n",
        "      self.activation = nn.ELU\n",
        "\n",
        "    if self.batch_norm == 'YES':\n",
        "      if self.architecture == 'DOUBLE':\n",
        "        self.num_filters = 8\n",
        "        for i in range(self.num_conv_layers):\n",
        "          self.layers.append(nn.Conv2d(self.input_filters,self.num_filters, self.conv_filter_size))\n",
        "          self.layers.append(nn.BatchNorm2d(self.num_filters))\n",
        "          self.layers.append(self.activation())\n",
        "          self.layers.append(nn.MaxPool2d(self.maxpool_filter_size, stride = 2))\n",
        "          self.layers.append(nn.Dropout(self.dropout))\n",
        "          self.input_filters = self.num_filters\n",
        "          self.num_filters = self.num_filters*2\n",
        "        \n",
        "      if self.architecture == 'HALF':\n",
        "        self.num_filters = 128\n",
        "        for i in range(self.num_conv_layers):\n",
        "          print(i)\n",
        "          self.layers.append(nn.Conv2d(self.input_filters,self.num_filters, self.conv_filter_size))\n",
        "          self.layers.append(nn.BatchNorm2d(self.num_filters))\n",
        "          self.layers.append(self.activation())\n",
        "          self.layers.append(nn.MaxPool2d(self.maxpool_filter_size, stride = 2))\n",
        "          self.layers.append(nn.Dropout(self.dropout))\n",
        "          self.input_filters = self.num_filters\n",
        "          print('input_filters', self.input_filters)\n",
        "          self.num_filters = int(self.num_filters/2)\n",
        "          print('number of filters', self.num_filters)\n",
        "      \n",
        "      if self.architecture == 'EQUAL':\n",
        "        for i in range(self.num_conv_layers):\n",
        "          self.layers.append(nn.Conv2d(self.input_filters,self.num_filters, self.conv_filter_size))\n",
        "          self.layers.append(nn.BatchNorm2d(self.num_filters))\n",
        "          self.layers.append(self.activation())\n",
        "          self.layers.append(nn.MaxPool2d(self.maxpool_filter_size, stride = 2))\n",
        "          self.layers.append(nn.Dropout(self.dropout))\n",
        "          self.input_filters = self.num_filters\n",
        "\n",
        "    if self.batch_norm == 'NO':\n",
        "      if self.architecture == 'DOUBLE':\n",
        "        self.num_filters = 8\n",
        "        for i in range(self.num_conv_layers):\n",
        "          self.layers.append(nn.Conv2d(self.input_filters,self.num_filters, self.conv_filter_size))\n",
        "          self.layers.append(self.activation())\n",
        "          self.layers.append(nn.MaxPool2d(self.maxpool_filter_size, stride = 2))\n",
        "          self.layers.append(nn.Dropout(self.dropout))\n",
        "          self.input_filters = self.num_filters\n",
        "          self.num_filters = self.num_filters*2\n",
        "        \n",
        "      if self.architecture == 'HALF':\n",
        "        self.num_filters = 128\n",
        "        for i in range(self.num_conv_layers):\n",
        "          self.layers.append(nn.Conv2d(self.input_filters,self.num_filters, self.conv_filter_size))\n",
        "          self.layers.append(self.activation())\n",
        "          self.layers.append(nn.MaxPool2d(self.maxpool_filter_size, stride = 2))\n",
        "          self.layers.append(nn.Dropout(self.dropout))\n",
        "          self.input_filters = self.num_filters\n",
        "          self.num_filters = self.num_filters/0.5\n",
        "      \n",
        "      if self.architecture == 'EQUAL':\n",
        "        for i in range(self.num_conv_layers):\n",
        "          self.layers.append(nn.Conv2d(self.input_filters,self.num_filters, self.conv_filter_size))\n",
        "          self.layers.append(self.activation())\n",
        "          self.layers.append(nn.MaxPool2d(self.maxpool_filter_size, stride = 2))\n",
        "          self.layers.append(nn.Dropout(self.dropout))\n",
        "          self.input_filters = self.num_filters\n",
        "\n",
        "   # Construct sequential module\n",
        "    self.cnn_model = nn.Sequential()\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      self.cnn_model.add_module(str(i), layer)\n",
        "  \n",
        "    output_dim = self.compute_output_dim(self.num_conv_layers, self.conv_filter_size, self.maxpool_filter_size)\n",
        "    input_dense = (output_dim**2)*self.input_filters\n",
        "\n",
        "    self.fc_model = nn.Sequential(\n",
        "        nn.Linear(input_dense, self.dense_neurons),\n",
        "        self.activation(),\n",
        "        nn.Linear(self.dense_neurons,10)\n",
        "    )\n",
        "  \n",
        "  def compute_output_dim(self, num_conv_layers, conv_filter_size, maxpool_filter_size):\n",
        "    input_size = image_dim\n",
        "    for i in range (num_conv_layers):\n",
        "      conv_output_dim = (input_size - conv_filter_size) + 1\n",
        "      maxpool_output_dim = np.floor((conv_output_dim - maxpool_filter_size)/2) + 1\n",
        "      input_size = maxpool_output_dim\n",
        "    return int(maxpool_output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.cnn_model(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    print(x.shape)\n",
        "    x = self.fc_model(x)\n",
        "    return(x)"
      ],
      "metadata": {
        "id": "uh0KG2terZOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN(\n",
        "    architecture = 'DOUBLE',\n",
        "    batch_norm = 'YES',\n",
        "    num_layers = 5,\n",
        "    num_filters = 32, \n",
        "    conv_filter_size = 5,\n",
        "    dropout = 0.2, \n",
        "    activation = 'LeakyReLU',\n",
        "    dense_neurons = 128).to(device)"
      ],
      "metadata": {
        "id": "BkqW63dVLslz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(dataloader):\n",
        "  total, correct_predictions = 0, 0\n",
        "  for data in dataloader:\n",
        "    X, Y = data\n",
        "    X, Y = X.to(device), Y.to(device)\n",
        "    Y_pred = model(X)\n",
        "    _, pred = torch.max(Y_pred.data, 1)\n",
        "    total += Y.size(0)\n",
        "    correct_predictions += (pred == Y).sum().item()\n",
        "    accuracy = (correct_predictions/total)*100\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "th4qGvAxLEYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers = 2)\n",
        "val_loader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers = 2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False, num_workers = 2)"
      ],
      "metadata": {
        "id": "jIHjZ3ZhtDFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#backprop\n",
        "loss_cr = nn.CrossEntropyLoss()\n",
        "la = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "Sxld9CPHrNHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_batch = []\n",
        "loss_epoch = []\n",
        "epochs = 5\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "      images, labels = data\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      la.zero_grad()\n",
        "      y_pred = model(images)\n",
        "      loss = loss_cr(y_pred, labels)\n",
        "      loss.backward()\n",
        "      la.step()\n",
        "      loss_batch.append(loss.item())\n",
        "  loss_epoch.append(loss.item())\n",
        "  accuracy_val = accuracy(val_loader)\n",
        "  accuracy_train = accuracy(train_loader)     \n",
        "  print('Epoch: %d/%d, Validation acc: %0.2f, Train acc: %0.2f' % (epoch, epochs,accuracy_val, accuracy_train ))\n",
        "  wandb.log({'loss_epoch': loss_epoch, 'accuracy_val':accuracy_val, 'accuracy_training':accuracy_training, 'epoch':epoch})\n",
        "plt.plot(loss_epoch)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SIULq5l_wyji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WANDB"
      ],
      "metadata": {
        "id": "p9TXE3qzi_52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_configuration = {\n",
        "  'method': 'grid',\n",
        "  'name': 'sweep',\n",
        "  'metric': {'goal': 'maximize', 'name': 'accuracy_val'},\n",
        "  'parameters': {\n",
        "      'batch_norm':{'values':['YES','NO']},\n",
        "      'num_layers': {'values': [5]},\n",
        "      'num_filters': {'values': [32]},\n",
        "      'conv_filter_size': {'values': [5]},\n",
        "      'dropout': {'values': [0.05,0.1,0.2]},\n",
        "      'activation':{'values':['ReLU', 'GELU', 'LeakyReLU', 'SiLU', 'Mish']},\n",
        "      'dense_neurons': {'values': [128]},\n",
        "      'learning_rate': {'values': [1e-4,1e-5,1e-6]},\n",
        "      'weight_decay': {'values': [0, 0.0005, 0.5]},\n",
        "      'optimizer': {'values': ['Adam', 'NAdam','RAdam', 'AdamW', 'SGD']},\n",
        "    } }"
      ],
      "metadata": {
        "id": "YfEsVKJDPFNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wandbsweeps():\n",
        "  wandb.init(project = 'CS6910_Assignment_2')\n",
        "  print('wandb initialize')\n",
        "  wandb.run.name = (\n",
        "        \"bn\"\n",
        "        + str(wandb.config.batch_norm)\n",
        "        + \"nf\"\n",
        "        + str(wandb.config.num_filters)\n",
        "        + \"fs\"\n",
        "        + str(wandb.config.conv_filter_size)\n",
        "        + \"do\"\n",
        "        + str(wandb.config.dropout)\n",
        "        + \"lr\"\n",
        "        + str(wandb.config.learning_rate)\n",
        "        + \"opt\"\n",
        "        + wandb.config.optimizer\n",
        "        + \"af\"\n",
        "        + str(wandb.config.activation)\n",
        "    )\n",
        "  print('wandb run initialize')\n",
        "  model = CNN(\n",
        "    batch_norm = wandb.config.batch_norm,\n",
        "    num_layers = wandb.config.num_layers,\n",
        "    num_filters = wandb.config.num_filters, \n",
        "    conv_filter_size = wandb.config.conv_filter_size,\n",
        "    dropout = wandb.config.dropout, \n",
        "    activation = wandb.config.activation,\n",
        "    dense_neurons = wandb.config.dense_neurons).to(device)\n",
        "  print('model')\n",
        "\n",
        "  def accuracy(dataloader):\n",
        "    total, correct_predictions = 0, 0\n",
        "    for data in dataloader:\n",
        "      X, Y = data\n",
        "      X, Y = X.to(device), Y.to(device)\n",
        "      Y_pred = model(X)\n",
        "      _, pred = torch.max(Y_pred.data, 1)\n",
        "      total += Y.size(0)\n",
        "      correct_predictions += (pred == Y).sum().item()\n",
        "      accuracy = (correct_predictions/total)*100\n",
        "    return accuracy\n",
        "  \n",
        "  ## Loss and optimizer ##  \n",
        "  loss_cr = nn.CrossEntropyLoss()\n",
        "  print('Loss')\n",
        "  if wandb.config.optimizer == 'Adam':\n",
        "    la = optim.Adam(model.parameters(), lr=wandb.config.learning_rate, weight_decay=wandb.config.weight_decay)\n",
        "  elif wandb.config.optimizer == 'NAdam':\n",
        "    la = optim.NAdam(model.parameters(), lr=wandb.config.learning_rate, weight_decay= wandb.config.weight_decay)\n",
        "  elif wandb.config.optimizer == 'RAdam':\n",
        "    la = optim.RAdam(model.parameters(), lr=wandb.config.learning_rate, weight_decay=wandb.config.weight_decay)\n",
        "  elif wandb.config.optimizer == 'AdamW':\n",
        "    la = optim.AdamW(model.parameters(), lr=wandb.config.learning_rate, weight_decay=wandb.config.weight_decay)\n",
        "  elif wandb.config.optimizer == 'SGD':\n",
        "    la = optim.SGD(model.parameters(), lr=wandb.config.learning_rate, weight_decay=wandb.config.weight_decay)\n",
        "  else:\n",
        "      raise ValueError(\"Invalid optimizer type.\")\n",
        "    \n",
        "  print('optim')\n",
        "  loss_batch = []\n",
        "  loss_epoch = []\n",
        "  epochs = 5\n",
        "  for i in tqdm(range(epochs)):\n",
        "    epoch = i+1\n",
        "    for j, data in enumerate(train_loader, 0):\n",
        "        images, labels = data\n",
        "        la.zero_grad()\n",
        "        y_pred = model(images)\n",
        "        loss = loss_cr(y_pred, labels)\n",
        "        loss.backward()\n",
        "        la.step()\n",
        "        loss_batch.append(loss.item())\n",
        "    loss_epoch.append(loss.item())\n",
        "    accuracy_val = accuracy(val_loader)\n",
        "    accuracy_train = accuracy(train_loader)\n",
        "    print('training loop')     \n",
        "    print('Epoch: %d/%d, Validation acc: %0.2f, Train acc: %0.2f' % (epoch, epochs,accuracy_val, accuracy_train ))\n",
        "    wandb.log({'loss_epoch': loss_epoch, 'accuracy_val':accuracy_val, 'accuracy_training':accuracy_train, 'epoch':epoch})\n",
        "  wandb.finish()\n",
        "\n",
        "sweep_id = wandb.sweep(sweep= sweep_configuration, project = 'CS6910_Assignment_2')\n",
        "wandb.agent(sweep_id, function = wandbsweeps)"
      ],
      "metadata": {
        "id": "P0qDw1HjWSA1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}